{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running LDA analysis on summarized (condensed) text generated from articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(1729)\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_TEXT = ['\\nFrench startup BlaBlaCar has announced that the company’s revenue grew by 71 percent in 2019 compared to 2018',\n",
    " 'Ouibus is now called BlaBlaBus',\n",
    " 'The big difference between 2019 and 2018 is that BlaBlaCar diversified its activity by offering bus rides as well as bus ticketing in some markets.\\nBlaBlaCar is still mostly known for its long-distance ride-sharing marketplace',\n",
    " 'On the other side of the marketplace, if you plan on driving across the country, you can list your ride on the platform to find passengers so that you don’t have to pay for gas and highway tolls by yourself.\\nIn November 2018, the company acquired Ouibus to become a marketplace for road travel, whether it’s by bus or by car',\n",
    " 'If you’re going from one city to another, you can find a car with an empty seat and book a ride in that car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(ORIGINAL_TEXT):\n",
    "    \"\"\"Polishes text\"\"\"\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    STOPWORDS.add(\"-\")\n",
    "    STOPWORDS.add(\"like\")\n",
    "    STOPWORDS.add(\"said\")\n",
    "    STOPWORDS.add(\"forward\")\n",
    "    STOPWORDS.add(\"time\")\n",
    "    frp = []\n",
    "    for i, c in enumerate(ORIGINAL_TEXT):\n",
    "        reg = c.lower()\n",
    "        reg = ' '.join(reg)\n",
    "        reg = ' '.join([word for word in c.split() if word not in STOPWORDS])\n",
    "        reg = re.sub('[^a-zA-Z]', ' ', reg)\n",
    "        reg = re.sub(r'\\s+', ' ', reg)\n",
    "        frp.append(reg)\n",
    "    return frp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLISHED_TEXT = pre_process_text(ORIGINAL_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['With experience designing previous apps extensive help APEX Entrepreneurship Center Wani decided develop app HitchHiqe aid students search long distance carpooling rides Blacksburg',\n",
       " 'Wani spent around hours working HitchHiqe said There lot challenges you re building something idea white board sometimes feels doesn t work',\n",
       " 'Wani first came contact APEX kickstarter event struck conversation presenter possibilities virtual reality',\n",
       " ' The algorithms include screening drivers anyone driver HitchHiqe unless registered vehicle Wani',\n",
       " 'Production app development began May ',\n",
       " 'It Wani started path app development eventually guided toward recent app HitchHiqe',\n",
       " 'The app saves students around minutes process',\n",
       " 'The response MIT overwhelmingly positive',\n",
       " ' In order register HitchHiqe need edu account as well There also safety concerns Facebook group creation app hopes address We actually got email one moderators VT parent s group Facebook isn t secure kids would endorse on our platform Wani said explaining positive results app s safety precautions',\n",
       " 'Wani also received aid pitching idea Hackathon MIT last semester',\n",
       " 'He also attribute success Virginia Tech s APEX Center help funding marketing',\n",
       " 'A couple features HitchHiqe come work MIT Watson AI computer system developed IBM capable answering questions using AI analytical software',\n",
       " 'He received Facebook notification fellow Hokie Sai Gurrapu early HitchHiqe s production Gurrapu expressed interest helping Wani develop app',\n",
       " 'They sent beta version Facebook companies receive feedback regarding app They eventually equipped efficient system precautions designed ensure safety Virginia Tech student drivers passengers',\n",
       " ' We talked one people there touch ever since really helpful Wani',\n",
       " 'Sometimes I feel I m young myself find way out Wani however hasn t spent entire process completely alone',\n",
       " 'Wani built beta version HitchHiqe three weeks',\n",
       " 'After tirelessly scrolling Virginia Tech s carpooling Facebook group search ride home Qasim Wani sophomore majoring computer engineering realized inefficient minute process truly',\n",
       " 'The app designed sort relevant carpooling offers Facebook collect one platform Virginia Tech students find rides home without go every single Facebook post message multiple people']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POLISHED_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incorporating stemming instead of lemmatization because of performance and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmantized = list(pd.Series(POLISHED_TEXT).map(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(lemmantized)\n",
    "dictionary.filter_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(sentence):\n",
    "    \"\"\"\n",
    "    Generates Frequencies and Occurances of words from a sentence:\n",
    "    1. Parameters : sentence (lemmantized version of the sentence, in list type).\n",
    "    2. Returns : \n",
    "        A. TDM (pandas Series object)\n",
    "        B. DataFrame (pandas TDM representation)\n",
    "    \"\"\"\n",
    "    occurance = {}\n",
    "    frequency = {}\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in frequency.keys():\n",
    "            frequency[word] = 1\n",
    "        else:\n",
    "            frequency[word] += 1\n",
    "    max_word_frequency = max(frequency.values())\n",
    "    for word in frequency.keys():\n",
    "        occurance[word] = frequency[word] / max_word_frequency\n",
    "    df = pd.DataFrame(data=[list(frequency.keys()), list(frequency.values()), list(occurance.values())]).T\n",
    "    df.columns = ['Word', 'Occurance', 'Frequency']\n",
    "    return df, frequency, occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tf_idf(paragraph):\n",
    "    tf_idf = []\n",
    "    for i, lem in enumerate(paragraph):\n",
    "        pd_df, frequency_words, occurance_words = generate_frequencies(lem)\n",
    "        temp = []\n",
    "        for occur, freq in zip(frequency_words.values(), occurance_words.values()):\n",
    "            temp.append(freq * np.log10(len(ORIGINAL_TEXT)/occur))\n",
    "        tf_idf.append(temp)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TD-IDF generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_idf = generate_tf_idf(lemmantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "lemmy_BOW = [dictionary.doc2bow(text) for text in lemmantized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(lemmy_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[lemmy_BOW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=3, id2word=dictionary, passes=2, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.013*\"minutes\" + 0.013*\"saves\" + 0.012*\"built\" + 0.012*\"weeks\" + 0.012*\"students\" + 0.011*\"semester\" + 0.011*\"pitching\" + 0.011*\"hackathon\" + 0.011*\"hitchhiqe\" + 0.011*\"version\"\n",
      "Topic: 1 \n",
      "Word: 0.014*\"began\" + 0.013*\"overwhelmingly\" + 0.013*\"people\" + 0.013*\"response\" + 0.012*\"spent\" + 0.012*\"touch\" + 0.012*\"talked\" + 0.012*\"production\" + 0.012*\"helpful\" + 0.012*\"development\"\n",
      "Topic: 2 \n",
      "Word: 0.013*\"gurrapu\" + 0.012*\"drivers\" + 0.012*\"safety\" + 0.011*\"facebook\" + 0.011*\"funding\" + 0.011*\"attribute\" + 0.011*\"success\" + 0.011*\"marketing\" + 0.010*\"registered\" + 0.010*\"algorithms\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(lemmy_BOW, minimum_probability=0.2, num_topics=5, id2word=dictionary, passes=3, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.031*\"tech\" + 0.031*\"virginia\" + 0.031*\"process\" + 0.030*\"wani\" + 0.017*\"success\" + 0.017*\"funding\" + 0.017*\"attribute\" + 0.017*\"marketing\" + 0.017*\"center\" + 0.017*\"help\"\n",
      "Topic: 1 \n",
      "Words: 0.073*\"wani\" + 0.039*\"hitchhiqe\" + 0.022*\"people\" + 0.021*\"development\" + 0.021*\"path\" + 0.021*\"recent\" + 0.020*\"started\" + 0.020*\"idea\" + 0.020*\"weeks\" + 0.020*\"semester\"\n",
      "Topic: 2 \n",
      "Words: 0.044*\"wani\" + 0.044*\"hitchhiqe\" + 0.023*\"work\" + 0.022*\"apex\" + 0.013*\"carpooling\" + 0.013*\"search\" + 0.013*\"spent\" + 0.013*\"help\" + 0.013*\"idea\" + 0.013*\"center\"\n",
      "Topic: 3 \n",
      "Words: 0.039*\"safety\" + 0.038*\"facebook\" + 0.027*\"positive\" + 0.027*\"precautions\" + 0.026*\"group\" + 0.015*\"response\" + 0.015*\"overwhelmingly\" + 0.015*\"wani\" + 0.015*\"drivers\" + 0.015*\"ensure\"\n",
      "Topic: 4 \n",
      "Words: 0.046*\"facebook\" + 0.033*\"gurrapu\" + 0.032*\"students\" + 0.018*\"production\" + 0.018*\"received\" + 0.018*\"develop\" + 0.018*\"expressed\" + 0.018*\"notification\" + 0.018*\"helping\" + 0.018*\"early\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
