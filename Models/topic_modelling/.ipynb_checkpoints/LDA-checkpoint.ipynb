{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running LDA analysis on summarized (condensed) text generated from articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(1729)\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_TEXT = ['It can take up to 15 seconds of laser fire to bring down a UAV or destroy its camera',\n",
    " 'This was not a sudden development but has been going on for most of the last decade',\n",
    " 'In 2010 the navy successfully tested this new laser weapon, which is actually six solid-state lasers acting in unison, to destroy a small UAV',\n",
    " 'In 2013 another test was run, under more realistic conditions',\n",
    " '\\nIsrael claims a breakthrough in the development of lasers that can be used to intercept mortar shells, UAVs and rockets',\n",
    " 'Most objects fired at Israel end up landing in unoccupied areas and the few objects that are dangerous are intercepted by missiles',\n",
    " 'Fire control systems for quickly, accurately and repeatedly aiming a laser have already been developed',\n",
    " 'Navy system already installed on one warship for several years and about to be installed on several more',\n",
    " 'This was crucial because knocking down UAVs is not something that the navy needs help with',\n",
    " 'It never worked, at least not in a practical sense',\n",
    " 'Even if ALT worked flawlessly it did not have enough energy to hit a launching missile from a safe (from enemy fire) distance',\n",
    " 'This has proved very effective.\\n\\nLaser Dome is described as using a solid-state electric laser at an effective range of 5,000 meters',\n",
    " 'Laser Dome combines multiple laser beams to obtain a useful amount of laser power at longer ranges',\n",
    " 'In 2018 LaWAS was moved to a large amphibious ship for continued testing and two more LaWAS are being built, for delivery and installation on two more ships in 2020',\n",
    " 'Army CLWS (Compact Laser Weapon System) which is currently only capable of handling UAVs',\n",
    " 'This is the tech that Laser Dome claims to have improved enough to destroy UAVs with one shot and at longer ranges.\\n\\nAnother example is a U.S',\n",
    " 'The manufacturer convinced the navy that it was just a matter of tweaking the technology to get the needed effectiveness',\n",
    " 'But the ability to do enough damage to disable boats or missiles that are over two kilometers distant meant the LaWS was worth mounting on a warship.\\n\\nLaWS may yet prove incapable of working under combat conditions, but so far this new development has kept passing tests',\n",
    " 'But in heaver sand storms performance was much reduced',\n",
    " 'In other words, LaWAS is still a work in progress.\\n\\nSuch was not the case with an earlier research effort using chemical lasers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(ORIGINAL_TEXT):\n",
    "    \"\"\"Polishes text\"\"\"\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    STOPWORDS.add(\"-\")\n",
    "    STOPWORDS.add(\"like\")\n",
    "    STOPWORDS.add(\"said\")\n",
    "    STOPWORDS.add(\"forward\")\n",
    "    STOPWORDS.add(\"time\")\n",
    "    frp = []\n",
    "    for i, c in enumerate(ORIGINAL_TEXT):\n",
    "        reg = c.lower()\n",
    "        reg = ' '.join(reg)\n",
    "        reg = ' '.join([word for word in c.split() if word not in STOPWORDS])\n",
    "        reg = re.sub('[^a-zA-Z]', ' ', reg)\n",
    "        reg = re.sub(r'\\s+', ' ', reg)\n",
    "        frp.append(reg)\n",
    "    return frp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLISHED_TEXT = pre_process_text(ORIGINAL_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It probably name earned semi human status forgot water last year s degree summer heat still managed thrive',\n",
       " ' It s pretty cool textile says Amanda Johnston who together expo s founder Nina Marenzi curated vast array fabrics offer',\n",
       " 'For Future Fabrics Expo London week cactus leather one array new ethical man made fibres display',\n",
       " 'Veganism exploded that s transcending materials seeing developed too Read more Could You Give Up Buying Clothes In Cactus pineapple apple leather show Future Fabrics Expo Paul Cochrane Image may contain Plant Human Person Fruit Food Pineapple Established Future Fabrics Expo Selfridges sustainable textiles edited selection best offerings market today',\n",
       " 'The event one stop shop environmentally conscious designers fashion industry one polluting planet it s relevant ever As result I m happy report I went London s Victoria House attend three day event buzzing',\n",
       " 'The expo organised The Sustainable Angle not for profit organisation initiates supports sustainable projects within fashion related industries acts hub fashion designers fabric sourcers students seeking sustainable fibres across global market',\n",
       " ' We ve also got apple leather grape leather pineapple leather',\n",
       " ' Because unless designer understands fabric environmentally friendly far less impact going understand need initiate change Paul Cochrane Image may contain Human Person Clothing Apparel Airport Sustainability hot topic fashion today industry constant state flux luxury high street brands attempt implement better practice fabric choices organic cotton supply chains manufacturing conditions',\n",
       " 'Panel talks seminars arranged partner Parley Oceans founder Cyrill Gutsch worked Stella McCartney Adidas offered engaging in depth discussions recycling textiles fashion could positive impact ocean',\n",
       " 'Meanwhile rails stuffed brim fabrics hive activity representatives British luxury brands including Wales Bonner Molly Goddard Mother Pearl sifting options clamouring thousands materials offer Paul Cochrane Image may contain Clothing Apparel Sleeve Long Sleeve Gown Robe Fashion Evening Dress Blouse From lab made silks fabric made charcoal vegan wools latter crafted calotropis plant medicinal weed grows abundance desert like lands hanger accompanied blurb detailed sustainable credentials every piece cloth',\n",
       " ' We spend huge amount research little blurbs says Marenzi',\n",
       " 'That verdant green plant covered tiny prickles likely nestled within chic ceramic plant pot',\n",
       " 'It doesn t die house plants often',\n",
       " 'This cactus friend Now consider wearing',\n",
       " 'Consider cactus shelf',\n",
       " ' We re seeing plethora really amazing leather alternatives area honeypot growth Johnston continues']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POLISHED_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incorporating stemming instead of lemmatization because of performance and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmantized = list(pd.Series(POLISHED_TEXT).map(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(lemmantized)\n",
    "dictionary.filter_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(sentence):\n",
    "    \"\"\"\n",
    "    Generates Frequencies and Occurances of words from a sentence:\n",
    "    1. Parameters : sentence (lemmantized version of the sentence, in list type).\n",
    "    2. Returns : \n",
    "        A. TDM (pandas Series object)\n",
    "        B. DataFrame (pandas TDM representation)\n",
    "    \"\"\"\n",
    "    occurance = {}\n",
    "    frequency = {}\n",
    "    for i, word in enumerate(sentence):\n",
    "        if word not in frequency.keys():\n",
    "            frequency[word] = 1\n",
    "        else:\n",
    "            frequency[word] += 1\n",
    "    max_word_frequency = max(frequency.values())\n",
    "    for word in frequency.keys():\n",
    "        occurance[word] = frequency[word] / max_word_frequency\n",
    "    df = pd.DataFrame(data=[list(frequency.keys()), list(frequency.values()), list(occurance.values())]).T\n",
    "    df.columns = ['Word', 'Occurance', 'Frequency']\n",
    "    return df, frequency, occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tf_idf(paragraph):\n",
    "    tf_idf = []\n",
    "    for i, lem in enumerate(paragraph):\n",
    "        pd_df, frequency_words, occurance_words = generate_frequencies(lem)\n",
    "        temp = []\n",
    "        for occur, freq in zip(frequency_words.values(), occurance_words.values()):\n",
    "            temp.append(freq * np.log10(len(ORIGINAL_TEXT)/occur))\n",
    "        tf_idf.append(temp)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TD-IDF generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_idf = generate_tf_idf(lemmantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "lemmy_BOW = [dictionary.doc2bow(text) for text in lemmantized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(lemmy_BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[lemmy_BOW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=3, id2word=dictionary, passes=2, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.008*\"week\" + 0.008*\"display\" + 0.008*\"ethical\" + 0.008*\"plant\" + 0.008*\"future\" + 0.007*\"chic\" + 0.007*\"verdant\" + 0.007*\"likely\" + 0.007*\"covered\" + 0.007*\"green\"\n",
      "Topic: 1 \n",
      "Word: 0.009*\"friend\" + 0.009*\"wearing\" + 0.009*\"sustainable\" + 0.007*\"event\" + 0.007*\"consider\" + 0.007*\"fabric\" + 0.007*\"fashion\" + 0.007*\"cactus\" + 0.007*\"seeing\" + 0.006*\"designers\"\n",
      "Topic: 2 \n",
      "Word: 0.012*\"plants\" + 0.012*\"shelf\" + 0.012*\"leather\" + 0.010*\"house\" + 0.010*\"consider\" + 0.009*\"marenzi\" + 0.009*\"says\" + 0.009*\"grape\" + 0.008*\"research\" + 0.008*\"spend\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(lemmy_BOW, minimum_probability=0.2, num_topics=5, id2word=dictionary, passes=3, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.023*\"fabrics\" + 0.023*\"array\" + 0.023*\"expo\" + 0.023*\"cactus\" + 0.022*\"founder\" + 0.012*\"offer\" + 0.012*\"says\" + 0.012*\"johnston\" + 0.012*\"marenzi\" + 0.012*\"curated\"\n",
      "Topic: 1 \n",
      "Words: 0.032*\"leather\" + 0.022*\"fabric\" + 0.012*\"little\" + 0.012*\"grape\" + 0.012*\"spend\" + 0.012*\"blurbs\" + 0.012*\"huge\" + 0.012*\"research\" + 0.012*\"marenzi\" + 0.012*\"says\"\n",
      "Topic: 2 \n",
      "Words: 0.016*\"event\" + 0.016*\"fashion\" + 0.016*\"sleeve\" + 0.009*\"designers\" + 0.009*\"house\" + 0.009*\"london\" + 0.009*\"report\" + 0.009*\"happy\" + 0.009*\"stop\" + 0.009*\"relevant\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"human\" + 0.021*\"cactus\" + 0.021*\"expo\" + 0.021*\"pineapple\" + 0.021*\"fabrics\" + 0.021*\"future\" + 0.012*\"year\" + 0.012*\"heat\" + 0.012*\"thrive\" + 0.012*\"degree\"\n",
      "Topic: 4 \n",
      "Words: 0.039*\"sustainable\" + 0.028*\"fashion\" + 0.027*\"plant\" + 0.015*\"fabric\" + 0.015*\"expo\" + 0.015*\"verdant\" + 0.015*\"nestled\" + 0.015*\"green\" + 0.015*\"covered\" + 0.015*\"prickles\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
