{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is text summarization.\n",
    "### Objective, to summarize an article and make sense off of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_frequency(sentence, average_sentence_word_count):\n",
    "    \"\"\"\n",
    "    Calculates the weighted frequency of a single sentence.\n",
    "    Parameters:\n",
    "    1. sentence. A string containing multiple words.\n",
    "    Returns : word_frequencies (type = dict) list of words and associative weights.\n",
    "    \"\"\"\n",
    "    word_frequencies = {}\n",
    "    if len(sentence.split(\" \")) < average_sentence_word_count:\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "        max_word_frequency = max(word_frequencies.values())   \n",
    "        for word in word_frequencies.keys():\n",
    "            word_frequencies[word] /= max_word_frequency\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_weighted_score(paragraph, average_word_count):\n",
    "    \"\"\"\n",
    "    Generates the weighted score of the entire text.\n",
    "    Uses calculate_sentence_frequency(paragraph[i]).\n",
    "    Parameters:\n",
    "    1. paragraph. A list of sentences.\n",
    "    Returns:\n",
    "    1. sentence_scores (type = dict) list of sentence and associative weights.\n",
    "    \"\"\"\n",
    "    sentence_scores = {}\n",
    "    for i, sent in enumerate(paragraph):\n",
    "        word_frequencies = calculate_sentence_frequency(paragraph[i], average_word_count)\n",
    "        for word in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():\n",
    "                sentence_scores[sent] =  word_frequencies[word]\n",
    "            else:\n",
    "                sentence_scores[sent] += word_frequencies[word]\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    STOPWORDS.add(\"-\")\n",
    "    \n",
    "    ORIGINAL_TEXT = \"\"\"\n",
    "    A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.\n",
    "    \"\"\"\n",
    "    \n",
    "    TESLA_TEXT = ORIGINAL_TEXT.lower().replace(\". \", \" qwertyuiop\")\n",
    "    TESLA_TEXT = re.sub('[^a-zA-Z]', ' ', TESLA_TEXT )\n",
    "    TESLA_TEXT = re.sub(r'\\s+', ' ', TESLA_TEXT)\n",
    "    TESLA_TEXT = TESLA_TEXT.split(\" qwertyuiop\")\n",
    "\n",
    "    average_sentence_word_count = len(TESLA_TEXT)\n",
    "    sum_word_count = 0\n",
    "    for c,text in enumerate(TESLA_TEXT):\n",
    "        TESLA_TEXT[c] = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "        sum_word_count += len(TESLA_TEXT[c].split(\" \"))\n",
    "\n",
    "    average_sentence_word_count = sum_word_count / average_sentence_word_count\n",
    "    \n",
    "    sentence_scores = get_text_weighted_score(TESLA_TEXT, average_sentence_word_count)\n",
    "    original_dict = {}\n",
    "    ORIGINAL_TEXT = ORIGINAL_TEXT.split(\". \")\n",
    "    for i, sentences in enumerate(sentence_scores.items()):\n",
    "        original_dict[ORIGINAL_TEXT[i]] = sentences[1]\n",
    "    sorted_sentences = sorted(original_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sorted_sentences = main()\n",
    "    final_list = []\n",
    "    for i, s in enumerate(sorted_sentences):\n",
    "        final_list.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
